\section{Block Packed Storage}\label{sec: memory layout}
Single-step updates are useful in small scale settings to understand the local behaviour of the random field or if numerical stability is an issue. For the analysis of a large number of evaluations, however, this isn't the most efficient solution as we can't rely on BLAS-3 operations in the update process. To avoid this drawback our implementation offers bulk updates, which admit a performance boost while also preserving theoretical accuracy.

To this end, let us assume we are always simulating $k$ new evaluations of the random field, which we denote by $Z_{i} = (Z_{i}^{(1)},...,Z_{i}^{(k)})$. Then, the lower triangular matrix \(\Lmatrix_n\) from Section~\ref{sec: incremental cholesky} can be represented by \(k\times k\) block matrices \(L_{i,j}\) as
\[
	\Lmatrix_n = \begin{pmatrix}
		L_{0,0} & \\
		L_{1,0} & L_{1,1} \\
		L_{2,0} & L_{2,1} & L_{2,2} \\
		\vdots & & & \ddots \\
		L_{n-1,0} & & \dots & & L_{n-1,n-1}
	\end{pmatrix}
\]
where all the matrices \(L_{0,0},L_{1,1},\dots, L_{n-1,n-1}\) have zeros above their diagonal,
while the other matrices are full matrices.

Now \(Z_{n+1}\) is a \(k\) dimensional random vector which we would like to simulate in bulk. For this purpose, observe that the covariance matrix of $(Z_1,...,Z_{n+1})$ is given by 
\[
	\Sigma_{n+1} = 	\begin{pmatrix} 
		\Sigma_n & \mathbf{c}_n \\
		\mathbf{c}_n^T 	& \mathbf{\sigma}_n^2
	\end{pmatrix},
\]
where $c_n,  \mathbf{\sigma}_n^2$ are now covariance matrices, i.e. $\mathbf{\sigma}_n^2 = \C(Z_{n+1})$ and
\[  \mathbf{c}_n = 
	\begin{pmatrix}
	C_0 \\ \vdots \\ C_{n-1}
	\end{pmatrix}  \quad \text{with} \quad 
	C_i = 
	\begin{pmatrix}
		\C(Z_i^{(1)},Z_{n+1}^{(1)}) & \hdots & \C(Z_i^{(1)}, Z_{n+1}^{(k)}) \\
		\vdots & \ddots & \vdots \\
		\C(Z_i^{(k)},Z_{n+1}^{(1)}) & \hdots & \C(Z_i^{(k)}, Z_{n+1}^{(k)})
	\end{pmatrix}.
		 \]
Similarly, $\Lmatrix_{n+1}$ becomes
\[ \Lmatrix_{n+1} = 
\begin{pmatrix}
	\Lmatrix_n & \\
	\mathbf{l}_n^T & \Lambda_n
\end{pmatrix} \quad \text{with}Â \quad \mathbf{l}_n^T = (L_{n,0},...,L_{n,n}), \]
where $\mathbf{l}_n$ and $\Lambda_n$ need to satisfy 
\[  \Lmatrix_n \mathbf{l}_n = \mathbf{c}_n, \quad \Lambda_n \Lambda_n^T + \mathbf{l}_n^T \mathbf{l}_n= \mathbf{\sigma}_n^2. \]
Hence, the procedure for a block update of $\Lmatrix_n$ is very similar to Algorithm~\ref{algo: sim Z_{n+1}}. Yet, its superiority becomes clear when exploiting the block structure of $\Lmatrix_n$ while determining $\mathbf{l}_n$: Considering the structure of $\Lmatrix_n$, the first block of $\mathbf{l}_n$, $L_{n,0}^T$, is given by 
\[ L_{n,0}^T = L_{0,0}^{-1}C_0, \]
which can be determined at low cost since $L_{0,0}$ is lower triangular. For the second block, $L_{n,1}$, we deduce the condition
\[ L_{1,0} L_{n,0} + L_{1,1} L_{n,1} = C_1. \]
Through recursion we arrrive at Algorithm~\ref{algo: solve linear equation} for determining $\mathbf{l}_n$.
{
	\newcommand{\xc}{{\color{red} C}}
	\newcommand{\xL}{{\color{violet} L}}
	\newcommand{\xl}{{\color{teal} \Gamma}}
	\begin{algorithm}
		\caption{\(\Lmatrix_n^{-1}c_n\) stored as new row resulting in \(\Lmatrix_{n+1}\)}	
		\label{algo: solve linear equation}
		\For{\(r = 0,\dots,n-1\)}{
			\(\xc \leftarrow C_r\)\tcp*[f]{red cache}\;
			\For{\(i = 0,\dots, r-1\)}{
				\(\xL \leftarrow L_{r,i}\)\tcp*[f]{violet cache}\;
				\(\xl \leftarrow L_{n,i}\)\tcp*[f]{teal cache}\;
				\(\xc \leftarrow \xc - \xL\xl^T\)\;
			}
			\(\xL \leftarrow L_{r,r}\)\tcp*[f]{
				lower triangular}\;
			\(\xc \leftarrow \xL^{-1} \xc\)\;
			\(L_{n,r} \leftarrow \xc^T\)\;
		}
	\end{algorithm}
}
All of the caches in Algorithm~\ref{algo: solve linear equation} are of size
\(k\times k\).  Our total cache needs are therefore \(3k^2\). Recycling our block vector of standard normal random values $Y_{1:n} = (Y_1,...,Y_n)$ and recalling that 
\[\Lambda \Lambda^T = \mathbf{\sigma}_n^2 - \sum_{i=0}^{n-1}L_{n,i}L_{n,i}^T, \quad Z_{1:(n+1)} = \begin{pmatrix}
	\Lmatrix_n Y_{1:n} \\
	\mathbf{l}_n^TY_{1:n} + \Lambda_n Y_{n+1}
\end{pmatrix}, \]
we implement the remaining steps of Algorithm~\ref{algo: sim Z_{n+1}} in Algorithm~\ref{algo:
calc lambda and simulate Z_{n+1}} by replacing the square root with a Cholesky decomposition. As the blue and violet caches only require
\(k\times 1\) space, it requires less than \(3k^2\) cache space for \(k\ge 2\).

{
	\newcommand{\xS}{{\color{red} S}}
	\newcommand{\xl}{{\color{teal} \Gamma}}
	\newcommand{\xY}{{\color{violet} Y}}
	\newcommand{\xZ}{{\color{blue} Z}}
	\begin{algorithm}
		\caption{Calculation of \(\lambda_n\) and Simulation of \(Z_{n+1}\)}	
		\label{algo: calc lambda and simulate Z_{n+1}}
		\(\xS \leftarrow \sigma_n^2\)\tcp*[f]{red cache}\;
		\(\xZ \leftarrow 0_{k\times 1}\)\tcp*[f]{blue cache}\;
		\For{\(i = 0,\dots,n-1\)}{
			\(\xl \leftarrow L_{n,i}\)\tcp*[f]{teal cache}\;
			\(\xS \leftarrow \xS - \xl\xl^T\)\;
			\(\xY \leftarrow Y_i\)\tcp*[f]{violet cache}\;
			\(\xZ \leftarrow \xZ + \xl^T \xY\)\;
		}
		\(L_{n,n} \leftarrow \chol(\xS)\)\tcp*[f]{lower triangular}\;
		\KwRet{\(\xZ\)}
	\end{algorithm}
}
